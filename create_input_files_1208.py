
import time
import os
import numpy as np
from imageio import imread
from skimage.transform import resize as imresize
from random import seed, choice, sample
import torch
import skimage.io as io
import clip
from PIL import Image
import pickle
import json
import os
from tqdm import tqdm
import argparse, random
def process_captions(flat_captions):
    processed_captions = []
    caplens = []

    for caption in flat_captions:
        # ç¡®ä¿æ¯ä¸ªæè¿°ä»¥å¥å·ç»“å°¾
        if not caption.strip().endswith('.'):
            caption += '.'
        
        # æ·»åŠ åˆ°å¤„ç†åçš„åˆ—è¡¨ä¸­
        processed_captions.append(caption)
        
        # è®¡ç®—æè¿°çš„é•¿åº¦ï¼ˆè¿™é‡Œå‡è®¾ä»¥ç©ºæ ¼åˆ†éš”çš„å•è¯æ•°ä¸ºé•¿åº¦ï¼‰
        words = caption.split()
        caplen = len(words)
        caplens.append(caplen)

    return processed_captions, caplens

# è°ƒç”¨
 # æ‰“å°ç¬¬äºŒä¸ªé”®å€¼å¯¹


def create_input_files(args,dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,
                       max_len=100):
    """
    Creates input files for training, validation, and test data.

    :param dataset: name of dataset, one of 'coco', 'flickr8k', 'flickr30k'
    :param karpathy_json_path: path of Karpathy JSON file with splits and captions
    :param image_folder: folder with downloaded images
    :param captions_per_image: number of captions to sample per image
    :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s
    :param output_folder: folder to save files
    :param max_len: don't sample captions longer than this length
    """

    # Read Karpathy JSON
    with open(karpathy_json_path, 'r') as j:
        data = json.load(j)

    template_path = "template.txt"  # æ›¿æ¢ä¸ºä½ çš„æ¨¡æ¿æ–‡ä»¶è·¯å¾„
    with open(template_path, "r") as file:
        template = file.read().strip() + " "  # åŠ è½½å¹¶å»æ‰é¦–å°¾ç©ºæ ¼

    # Read image paths and captions for each image
    train_image_paths = []
    train_image_captions = []
    train_image_changeflag = []
    train_image_id = []

    val_image_paths = []
    val_image_captions = []
    val_image_changeflag = []
    val_image_id = []

    test_image_paths = []
    test_image_captions = []
    test_image_changeflag = []
    test_image_id = []

    # word_freq = Counter()  # åˆ›å»ºä¸€ä¸ªç©ºçš„Counterç±»(è®¡æ•°

    train_num=0
    for img in data['images']:
        captions = []
        for c in img['sentences']:
            # Update word frequency
            # word_freq.update(c['tokens'])   # å…¶ä¸­c['tokens']æ˜¯ä¸€ä¸ªå¾ˆå¤šå•è¯ç»„æˆçš„å¥å­â€˜åˆ—è¡¨â€™
            # if len(c['tokens']) <= max_len:
            captions.append(c['raw'].replace(' .','').replace('.',''))  #æŠŠå¥å°¾çš„å¥å·å»æ‰
            # print('captions', captions)
            # exit()
        if len(captions) == 0:
            continue

        if dataset == 'LEVIR_CC':
            path1 = os.path.join(image_folder, img['split'], 'A', img['filename'])
            path2 = os.path.join(image_folder, img['split'], 'B', img['filename'])
            path = [path1, path2]
            changeflag = img['changeflag']
            image_id = img['imgid']

        if img['split'] in {'train'}:
            train_image_paths.append(path)
            train_image_captions.append(captions)
            train_image_changeflag.append(changeflag)
            train_image_id.append(image_id)
            # train_num = train_num+1
        elif img['split'] in {'val'}:
            val_image_paths.append(path)
            val_image_captions.append(captions)
            val_image_changeflag.append(changeflag)
            val_image_id.append(image_id)


        elif img['split'] in {'test'}:
            test_image_paths.append(path)
            test_image_captions.append(captions)
            test_image_changeflag.append(changeflag)
            test_image_id.append(image_id)
            # print(f'TEST DATA - imgid:{image_id},changeflage : {changeflag}')


    # Sanity check
    assert len(train_image_paths) == len(train_image_captions)
    assert len(val_image_paths) == len(val_image_captions)
    assert len(test_image_paths) == len(test_image_captions)


    # Create a base/root name for all output files
    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img'

    # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files
    seed(123)
    device = torch.device('cuda:0')
    # print('train_image_captions', train_image_captions)
    test_caption = 'data/LEVIR_CC/v1/TEST_retrived_caps_v2.json'
    train_caption = 'data/LEVIR_CC/v1/TRAIN_retrived_caps_v2.json'
    val_caption = 'data/LEVIR_CC/v1/VAL_retrived_caps_v2.json'
    for impaths, imcaps, imchangeflag, imgid, caption_path, split, in [
                                   (test_image_paths, test_image_captions, test_image_changeflag, test_image_id, test_caption, 'TEST'),
                                   (train_image_paths, train_image_captions, train_image_changeflag, train_image_id, train_caption, 'TRAIN'),
                                    (val_image_paths, val_image_captions, val_image_changeflag, val_image_id, val_caption, 'VAL')]:
        # print('imcaps', imcaps)
        out_path = os.path.join(output_folder, split +'_' + base_filename + '.pkl')
        caps_path = caption_path
        feature_list = []
        enc_captions = []
        enco_captions = []
        caplens = []
        
        retrieved_caps = json.load(open(caps_path))
        counter = 0
        n = 10
        # print('retrieved_caps', type(retrieved_caps))
        # print('retrieved_caps', retrieved_caps)


        for i, path in enumerate(tqdm(impaths)):
            # if len(imchangeflag) <= i:
            #     print(f"Error: imchangeflag is empty or out of index at i={i}")
            #     continue  # é¿å…æŠ¥é”™
            # print(f"Processing index {i}, imgid={imgid[i]}, changeflag={imchangeflag[i]}")
            imgid[i] = str(imgid[i])
            filled_templates = []
            counter += 1
            # print('impaths[i], i, imgid[i] ==============', impaths[i], i, imgid[i])

            # print('imcaps[i]', imcaps[i])
            # if 'the scene is the same as before' in imcaps[i] and split == 'TRAIN':
            #     continue
            # Sample captions
            #å¤„ç†captionsï¼šå¦‚æœcaptionsä¸è¶³5ä¸ª ç”¨å·²æœ‰çš„å¡«å……æ»¡äº”ä¸ª
            #           å¦‚æœcaptionså¤Ÿ5ä¸ª çœ‹æ˜¯ä¸æ˜¯trainæ•°æ®é›†ã€‚
            #           å¦‚æœä¸æ˜¯trainæ•°æ®é›† æˆ–è€…æ˜¯trainæ•°æ®é›†ä½†å›¾ç‰‡å‘ç”Ÿäº†æ”¹åŠ¨ ä¸ç®¡ã€‚
            #           å¦åˆ™ï¼ˆæ˜¯trainæ•°æ®é›†åˆ‡å›¾ç‰‡æ²¡æ”¹å˜ å°±è¿™ä¸€ç§æƒ…å†µ éœ€è¦æŠŠ5ä¸ªcaptionå…¨éƒ¨å˜æˆ 'the scene is the same as before'ï¼‰

            if len(imcaps[i]) < captions_per_image:
                captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]

            if split == 'TEST':
                # if not imcaps[i]:
                #     print(f'error: imcaps[{i}] is empty')
                #     continue
                # for nochanged image pairs, just use one kind of nochanged captions during the training
                if imchangeflag[i] == 0:
                        # å¡«å…… || éƒ¨åˆ†
                    infix = "\n\n".join(["the scene is the same as before"] * 4)  # å¤åˆ¶4æ¬¡ï¼Œæ¯æ¬¡æ¢è¡Œ
                    template_0 = template.replace("||", infix)  # æ›¿æ¢ || å ä½ç¬¦

                    # å¡«å…… ** éƒ¨åˆ†
                    
                    j = 0
                    while j <= k:  # ç¡®ä¿åªå–å‰äº”ä¸ªå…ƒç´ 
                        # æ›¿æ¢æ¨¡æ¿ä¸­çš„ ** ä¸º caption
                        filled_template = template_0.replace("**", imcaps[i][j])
                        filled_templates.append(filled_template)
                        j += 1  # å°†å¡«å……åçš„æ¨¡æ¿åŠ å…¥ç»“æœåˆ—è¡¨
                    # print("Filled Templates:\n", filled_templates)
                    enc_captions.append(filled_templates)


                    # æ‰“å°ç»“æœ
                    # print("enc_captions:\n", enc_captions)

                else:
                    # print_nth_pair(retrieved_caps, 1) 
                    # å¡«å……||éƒ¨åˆ†
                    print(f"ğŸ” Checking imgid[{i}] = {imgid[i]}")
                    print(f"ğŸ” retrieved_caps keys (first 10): {list(retrieved_caps.keys())[:10]}")
                    if imgid[i] in retrieved_caps:
                        value_of_retrieved_caps = retrieved_caps[imgid[i]]  # è·å–å¯¹åº”çš„ value
                        caption_of_retrieved_caps = [lst[0] for lst in value_of_retrieved_caps]
                        merged_captions = "\n".join(caption_of_retrieved_caps)
                        template_1 = template.replace("||", merged_captions)  # æ›¿æ¢ || å ä½ç¬¦
                        # print("Filled Template:\n", template_1)
                    else:
                        print(f"âŒ imgid[{i}] = {imgid[i]} NOT found in retrieved_caps!")

                    # å¡«å…… ** éƒ¨åˆ†
                    filled_templates = []
                    j = 0
                    while j <= k:  # ç¡®ä¿åªå–å‰äº”ä¸ªå…ƒç´ 
                        # æ›¿æ¢æ¨¡æ¿ä¸­çš„ ** ä¸º caption
                        filled_template = template_1.replace("**", imcaps[i][j])
                        filled_templates.append(filled_template)
                        j += 1  # å°†å¡«å……åçš„æ¨¡æ¿åŠ å…¥ç»“æœåˆ—è¡¨
                    enc_captions.append(filled_templates)
                    # æ‰“å°ç»“æœæµ‹è¯•
                    # print("Filled Template:\n", filled_template)
                    # print("enc_captions:\n", enc_captions)

            # Sanity check
            assert len(captions) == captions_per_image

            # Read images
            if dataset =='LEVIR_CC':
                ori_img_A = io.imread(impaths[i][0])
                ori_img_B = io.imread(impaths[i][1])
                images = {'ori_img': [ori_img_A, ori_img_B], 'changeflag': imchangeflag[i]}
            else:
                print("Error")

            feature_list.append(images)



        # Sanity check
        # assert len(feature_list) * captions_per_image == len(enc_captions) == len(caplens)
        #æŠŠå›¾ç‰‡å¯¹ï¼ˆé‡Œé¢åŒ…å«ä¸¤ä¸ªå›¾ç‰‡å’Œæ˜¯å¦æ”¹å˜çš„flagï¼‰,åŠ ä¸Šäº†å¥å·çš„captionsï¼Œcaptionså¥å­çš„é•¿åº¦ å†™è¿›æ–‡ä»¶ä¸­
            # print("enco_captions:\n", enco_captions)
        flat_captions = [item for sublist in enc_captions for item in sublist]
        processed_captions, caplens = process_captions(flat_captions)

        print('flat_captions', flat_captions)
        print('caplens', caplens)
        with open(out_path, 'wb') as f:
            pickle.dump({"images": feature_list, "captions": processed_captions, 'caplens': caplens}, f)

if __name__ == '__main__':

    k = 4#num of relative caps
    # print('create_input_files START at: ', time.strftime("%m-%d  %H : %M : %S", time.localtime(time.time())))
    parser = argparse.ArgumentParser()
    parser.add_argument('--clip_model_type', default="ViT-B/32", choices=('RN50', 'RN101', 'RN50x4', 'ViT-B/32'))
    args = parser.parse_args()

    create_input_files(args, dataset='LEVIR_CC',
                       karpathy_json_path=r'./data/LEVIR_CC/LevirCCcaptions_v1.json',
                       image_folder=r'./data/LEVIR_CC/images',
                       captions_per_image=5,
                       min_word_freq=5,
                       output_folder=r'./data/LEVIR_CC/v2',
                       max_len=50)

    # print('create_input_files END at: ', time.strftime("%m-%d  %H : %M : %S", time.localtime(time.time())))