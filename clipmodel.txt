CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): ResidualAttentionBlock(
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (6): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (7): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (8): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (9): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (10): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (11): ResidualAttentionBlock(
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)

Compose(
    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    <function _convert_image_to_rgb at 0x0000027A110B3920>
    ToTensor()
    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))
)


model
class_embedding_classifier_changeflag torch.Size([1, 1536])
class_embedding_A torch.Size([1, 768])
class_embedding_B torch.Size([1, 768])
l_embedding.weight torch.Size([50, 768])
w_embedding.weight torch.Size([7, 384])
h_embedding.weight torch.Size([7, 384])
temporal_embedding.weight torch.Size([2, 768])
transformer_encoder.layers.0.self_attn.in_proj_weight torch.Size([2304, 768])
transformer_encoder.layers.0.self_attn.in_proj_bias torch.Size([2304])
transformer_encoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])
transformer_encoder.layers.0.self_attn.out_proj.bias torch.Size([768])
transformer_encoder.layers.0.linear1.weight torch.Size([3072, 768])
transformer_encoder.layers.0.linear1.bias torch.Size([3072])
transformer_encoder.layers.0.linear2.weight torch.Size([768, 3072])
transformer_encoder.layers.0.linear2.bias torch.Size([768])
transformer_encoder.layers.0.norm1.weight torch.Size([768])
transformer_encoder.layers.0.norm1.bias torch.Size([768])
transformer_encoder.layers.0.norm2.weight torch.Size([768])
transformer_encoder.layers.0.norm2.bias torch.Size([768])
transformer_encoder.layers.1.self_attn.in_proj_weight torch.Size([2304, 768])
transformer_encoder.layers.1.self_attn.in_proj_bias torch.Size([2304])
transformer_encoder.layers.1.self_attn.out_proj.weight torch.Size([768, 768])
transformer_encoder.layers.1.self_attn.out_proj.bias torch.Size([768])
transformer_encoder.layers.1.linear1.weight torch.Size([3072, 768])
transformer_encoder.layers.1.linear1.bias torch.Size([3072])
transformer_encoder.layers.1.linear2.weight torch.Size([768, 3072])
transformer_encoder.layers.1.linear2.bias torch.Size([768])
transformer_encoder.layers.1.norm1.weight torch.Size([768])
transformer_encoder.layers.1.norm1.bias torch.Size([768])
transformer_encoder.layers.1.norm2.weight torch.Size([768])
transformer_encoder.layers.1.norm2.bias torch.Size([768])
transformer_encoder_classifier.layers.0.self_attn.in_proj_weight torch.Size([4608, 1536])
transformer_encoder_classifier.layers.0.self_attn.in_proj_bias torch.Size([4608])
transformer_encoder_classifier.layers.0.self_attn.out_proj.weight torch.Size([1536, 1536])
transformer_encoder_classifier.layers.0.self_attn.out_proj.bias torch.Size([1536])
transformer_encoder_classifier.layers.0.linear1.weight torch.Size([3072, 1536])
transformer_encoder_classifier.layers.0.linear1.bias torch.Size([3072])
transformer_encoder_classifier.layers.0.linear2.weight torch.Size([1536, 3072])
transformer_encoder_classifier.layers.0.linear2.bias torch.Size([1536])
transformer_encoder_classifier.layers.0.norm1.weight torch.Size([1536])
transformer_encoder_classifier.layers.0.norm1.bias torch.Size([1536])
transformer_encoder_classifier.layers.0.norm2.weight torch.Size([1536])
transformer_encoder_classifier.layers.0.norm2.bias torch.Size([1536])
transformer_encoder_classifier.layers.1.self_attn.in_proj_weight torch.Size([4608, 1536])
transformer_encoder_classifier.layers.1.self_attn.in_proj_bias torch.Size([4608])
transformer_encoder_classifier.layers.1.self_attn.out_proj.weight torch.Size([1536, 1536])
transformer_encoder_classifier.layers.1.self_attn.out_proj.bias torch.Size([1536])
transformer_encoder_classifier.layers.1.linear1.weight torch.Size([3072, 1536])
transformer_encoder_classifier.layers.1.linear1.bias torch.Size([3072])
transformer_encoder_classifier.layers.1.linear2.weight torch.Size([1536, 3072])
transformer_encoder_classifier.layers.1.linear2.bias torch.Size([1536])
transformer_encoder_classifier.layers.1.norm1.weight torch.Size([1536])
transformer_encoder_classifier.layers.1.norm1.bias torch.Size([1536])
transformer_encoder_classifier.layers.1.norm2.weight torch.Size([1536])
transformer_encoder_classifier.layers.1.norm2.bias torch.Size([1536])
transformer_encoder_classifier.layers.2.self_attn.in_proj_weight torch.Size([4608, 1536])
transformer_encoder_classifier.layers.2.self_attn.in_proj_bias torch.Size([4608])
transformer_encoder_classifier.layers.2.self_attn.out_proj.weight torch.Size([1536, 1536])
transformer_encoder_classifier.layers.2.self_attn.out_proj.bias torch.Size([1536])
transformer_encoder_classifier.layers.2.linear1.weight torch.Size([3072, 1536])
transformer_encoder_classifier.layers.2.linear1.bias torch.Size([3072])
transformer_encoder_classifier.layers.2.linear2.weight torch.Size([1536, 3072])
transformer_encoder_classifier.layers.2.linear2.bias torch.Size([1536])
transformer_encoder_classifier.layers.2.norm1.weight torch.Size([1536])
transformer_encoder_classifier.layers.2.norm1.bias torch.Size([1536])
transformer_encoder_classifier.layers.2.norm2.weight torch.Size([1536])
transformer_encoder_classifier.layers.2.norm2.bias torch.Size([1536])
transformer_decoder.layers.0.self_attn.in_proj_weight torch.Size([2304, 768])
transformer_decoder.layers.0.self_attn.in_proj_bias torch.Size([2304])
transformer_decoder.layers.0.self_attn.out_proj.weight torch.Size([768, 768])
transformer_decoder.layers.0.self_attn.out_proj.bias torch.Size([768])
transformer_decoder.layers.0.multihead_attn.in_proj_weight torch.Size([2304, 768])
transformer_decoder.layers.0.multihead_attn.in_proj_bias torch.Size([2304])
transformer_decoder.layers.0.multihead_attn.out_proj.weight torch.Size([768, 768])
transformer_decoder.layers.0.multihead_attn.out_proj.bias torch.Size([768])
transformer_decoder.layers.0.linear1.weight torch.Size([1536, 768])
transformer_decoder.layers.0.linear1.bias torch.Size([1536])
transformer_decoder.layers.0.linear2.weight torch.Size([768, 1536])
transformer_decoder.layers.0.linear2.bias torch.Size([768])
transformer_decoder.layers.0.norm1.weight torch.Size([768])
transformer_decoder.layers.0.norm1.bias torch.Size([768])
transformer_decoder.layers.0.norm2.weight torch.Size([768])
transformer_decoder.layers.0.norm2.bias torch.Size([768])
transformer_decoder.layers.0.norm3.weight torch.Size([768])
transformer_decoder.layers.0.norm3.bias torch.Size([768])
classifier_projection.weight torch.Size([2, 1536])
classifier_projection.bias torch.Size([2])
conv_dif.0.weight torch.Size([384, 768, 3, 3])
conv_dif.0.bias torch.Size([384])
conv_dif.1.weight torch.Size([384])
conv_dif.1.bias torch.Size([384])
conv_dif.1.running_mean torch.Size([384])
conv_dif.1.running_var torch.Size([384])
conv_dif.1.num_batches_tracked torch.Size([])
linear_dif.weight torch.Size([2, 384])
linear_dif.bias torch.Size([2])
pre_linear.weight torch.Size([768, 768])
pre_linear.bias torch.Size([768])
CBAM.mlp.0.weight torch.Size([192, 768, 1, 1])
CBAM.mlp.2.weight torch.Size([768, 192, 1, 1])
CBAM.conv.weight torch.Size([1, 2, 7, 7])
CBAM.preflag_linear.weight torch.Size([2, 768])
CrossTransformer.0.attention.in_proj_weight torch.Size([2304, 768])
CrossTransformer.0.attention.in_proj_bias torch.Size([2304])
CrossTransformer.0.attention.out_proj.weight torch.Size([768, 768])
CrossTransformer.0.attention.out_proj.bias torch.Size([768])
CrossTransformer.0.attention2.in_proj_weight torch.Size([2304, 768])
CrossTransformer.0.attention2.in_proj_bias torch.Size([2304])
CrossTransformer.0.attention2.out_proj.weight torch.Size([768, 768])
CrossTransformer.0.attention2.out_proj.bias torch.Size([768])
CrossTransformer.0.norm1.weight torch.Size([768])
CrossTransformer.0.norm1.bias torch.Size([768])
CrossTransformer.0.norm2.weight torch.Size([768])
CrossTransformer.0.norm2.bias torch.Size([768])
CrossTransformer.0.norm3.weight torch.Size([768])
CrossTransformer.0.norm3.bias torch.Size([768])
CrossTransformer.0.linear1.weight torch.Size([3072, 768])
CrossTransformer.0.linear1.bias torch.Size([3072])
CrossTransformer.0.linear2.weight torch.Size([768, 3072])
CrossTransformer.0.linear2.bias torch.Size([768])
CrossTransformer.1.attention.in_proj_weight torch.Size([2304, 768])
CrossTransformer.1.attention.in_proj_bias torch.Size([2304])
CrossTransformer.1.attention.out_proj.weight torch.Size([768, 768])
CrossTransformer.1.attention.out_proj.bias torch.Size([768])
CrossTransformer.1.attention2.in_proj_weight torch.Size([2304, 768])
CrossTransformer.1.attention2.in_proj_bias torch.Size([2304])
CrossTransformer.1.attention2.out_proj.weight torch.Size([768, 768])
CrossTransformer.1.attention2.out_proj.bias torch.Size([768])
CrossTransformer.1.norm1.weight torch.Size([768])
CrossTransformer.1.norm1.bias torch.Size([768])
CrossTransformer.1.norm2.weight torch.Size([768])
CrossTransformer.1.norm2.bias torch.Size([768])
CrossTransformer.1.norm3.weight torch.Size([768])
CrossTransformer.1.norm3.bias torch.Size([768])
CrossTransformer.1.linear1.weight torch.Size([3072, 768])
CrossTransformer.1.linear1.bias torch.Size([3072])
CrossTransformer.1.linear2.weight torch.Size([768, 3072])
CrossTransformer.1.linear2.bias torch.Size([768])


self.gpt_decoder
Out[3]: 
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Sequential()
)